version: '3.8'

services:

  # ===================
  # Zookeeper & Kafka
  # ===================
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    ports:
      - "9092:9092"

  # ===================
  # MongoDB & Interface Web
  # ===================
  mongodb:
    image: mongo:6.0
    container_name: mongodb
    ports:
      - "27017:27017"
    volumes:
      - mongo-data:/data/db

  mongo-express:
    image: mongo-express:latest
    container_name: mongo-express
    restart: always
    ports:
      - "8082:8081" # Accessible sur http://localhost:8082
    environment:
      - ME_CONFIG_MONGODB_SERVER=mongodb
      - ME_CONFIG_MONGODB_PORT=27017
      - ME_CONFIG_MONGODB_ENABLE_ADMIN=true
    depends_on:
      - mongodb

  # ===================
  # Streamlit Dashboard
  # ===================
  dashboard:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: streamlit-dashboard
    environment:
      - MONGO_URI=mongodb://mongodb:27017/
    ports:
      - "8501:8501"
    depends_on:
      - mongodb
    volumes:
      - ./apps:/opt/spark/apps
    command: >
      bash -c "pip install streamlit pymongo pandas && 
               streamlit run /opt/spark/apps/app_visu.py --server.port=8501 --server.address=0.0.0.0"

  # ===================
  # Spark Cluster (Haute Disponibilit√©)
  # ===================
  spark-master-1:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-master-1
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    environment:
      - SPARK_DAEMON_JAVA_OPTS=-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=zookeeper:2181
    ports:
      - "8080:8080" # Spark Master Web UI
      - "7077:7077" # Spark Master Port
      - "4040:4040" # Spark Application UI (Streaming Monitoring)
    depends_on:
      - zookeeper
    volumes:
      - ./data:/opt/spark/data
      - ./apps:/opt/spark/apps

  spark-master-2:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-master-2
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    environment:
      - SPARK_DAEMON_JAVA_OPTS=-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=zookeeper:2181
    ports:
      - "8081:8080" # Spark Master 2 Web UI
      - "7078:7077"
    depends_on:
      - zookeeper
    volumes:
      - ./data:/opt/spark/data
      - ./apps:/opt/spark/apps

  spark-worker-1: &worker_base
    build:
      context: .
      dockerfile: Dockerfile.spark
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master-1:7077,spark-master-2:7077
    depends_on:
      - spark-master-1
      - spark-master-2
    volumes:
      - ./data:/opt/spark/data
      - ./apps:/opt/spark/apps

  spark-worker-2: { <<: *worker_base, container_name: spark-worker-2 }
  spark-worker-3: { <<: *worker_base, container_name: spark-worker-3 }
  spark-worker-4: { <<: *worker_base, container_name: spark-worker-4 }
  spark-worker-5: { <<: *worker_base, container_name: spark-worker-5 }

  # ===================
  # Producteur (Edge Node)
  # ===================
  corridor-edge-node:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: corridor-edge-node
    depends_on:
      - kafka
    volumes:
      - ./apps:/opt/spark/apps
    command: python3 /opt/spark/apps/producer.py

volumes:
  mongo-data: