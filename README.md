# â›½ Corridor Service Analytics: Real-Time Multi-Service Pipeline
[![Spark](https://img.shields.io/badge/Apache_Spark-3.5.0-orange?style=flat&logo=apachespark)](https://spark.apache.org/)
[![Kafka](https://img.shields.io/badge/Apache_Kafka-7.5.0-black?style=flat&logo=apachekafka)](https://kafka.apache.org/)
[![MongoDB](https://img.shields.io/badge/MongoDB-6.0-green?style=flat&logo=mongodb)](https://www.mongodb.com/)
[![Docker](https://img.shields.io/badge/Docker-Orchestrated-blue?style=flat&logo=docker)](https://www.docker.com/)

## ğŸ“ PrÃ©sentation
Ce projet dÃ©ploie une **Architecture Lambda** complÃ¨te pour le monitoring de l'expÃ©rience client au sein des stations-service au Mali. Le systÃ¨me analyse la performance de l'ensemble des pÃ´les d'activitÃ© : Carburant, Boutique, Lavage et Gaz.

## ğŸ—ï¸ Architecture du Pipeline (Data Flow)
1. **Ingestion Layer** : `Python Producer` â” `Apache Kafka`
2. **Speed Layer (Temps RÃ©el)** : `Spark Structured Streaming` â” `Sentiment Analysis` â” `MongoDB`
3. **Batch Layer (Data Lake)** : `JSON Raw` â” `Spark Compaction` â” `Parquet Format`

---

## ğŸš€ DÃ©ploiement

### 1. Lancement de l'infrastructure
```bash
docker-compose up -d --build


2. DÃ©marrage du Pipeline de Traitement
Bash
docker exec -it spark-master-1 spark-submit \
  --master spark://spark-master-1:7077,spark-master-2:7077 \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.mongodb.spark:mongo-spark-connector_2.12:10.3.0 \
  /opt/spark/apps/sentiment_analysis.py


3. Compactage vers le Data Lake (Batch Layer)
Pour optimiser le stockage et prÃ©parer les donnÃ©es pour le Machine Learning :

Bash
docker exec -it spark-master-1 spark-submit /opt/spark/apps/compact_to_parquet.py


ğŸ“Š Points d'accÃ¨s Monitoring
ğŸ–¥ï¸ Dashboard Streamlit : http://localhost:8501

âš™ï¸ Spark Master UI : http://localhost:8080

ğŸƒ Mongo Express : http://localhost:8082

ğŸ’¡ DÃ©fis Techniques RelevÃ©s
Haute DisponibilitÃ© : Cluster Spark configurÃ© avec 2 Masters (via Zookeeper) et 5 Workers pour garantir la rÃ©silience.

SchÃ©ma Ã‰volutif : Ingestion JSON flexible permettant de traiter des services variÃ©s (Lavage, Gaz, Boutique) sans modification du code.

Optimisation Storage : Migration vers le format Parquet (stockage colonnaire) pour rÃ©duire l'empreinte disque et accÃ©lÃ©rer les requÃªtes analytiques.